{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "administrative-message",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ready-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os   \n",
    "import torch\n",
    "from torch import empty\n",
    "torch.set_grad_enabled(False)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "harmful-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    '''\n",
    "    Compute forward pass from an input tensor and return a tensor\n",
    "    or a tuple of tensors as output\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        '''\n",
    "        should get as input a tensor or a tuple of tensors containing the \n",
    "        gradient of the loss with respect to the module’s output, accumulate \n",
    "        the gradient wrt the parameters, and return a tensor or a tuple of\n",
    "        tensors containing the gradient of the loss wrt the module’s input.\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def params(self):\n",
    "        '''\n",
    "        param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor\n",
    "        of same size. This list should be empty for parameterless modules (e.g. ReLU).\n",
    "\n",
    "        '''\n",
    "        return []\n",
    "        \n",
    "    def reset_params(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sufficient-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.init import xavier_normal_, xavier_normal\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.epsilon = 1e-3\n",
    "        self.x = 0\n",
    "\n",
    "        # Initialize weights\n",
    "        self.w = xavier_normal_(torch.empty(self.dim_out, self.dim_in))\n",
    "        self.b = torch.empty(self.dim_out).normal_(0, self.epsilon)\n",
    "\n",
    "        # Initialize gradient\n",
    "        self.dl_dw = torch.empty(self.w.size())\n",
    "        self.dl_db = torch.empty(self.b.size())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.x.mm(self.w.t()) + self.b\n",
    "\n",
    "\n",
    "    def backward(self, grad):\n",
    "        ds_dx = self.w.t()\n",
    "\n",
    "        # do the same for every batch (batch dim becomes 1)\n",
    "        dl_dx = ds_dx @ grad.t()\n",
    "\n",
    "        # put batch dim back to 0\n",
    "        dl_dx = dl_dx.t()\n",
    "\n",
    "        # sum over all the outer product between (grad_1 * x_1^T) (_1 denotes not using mini-batches)\n",
    "        self.dl_dw.add_(grad.t() @ self.x)\n",
    "\n",
    "        # sum over the batch\n",
    "        self.dl_db.add_(grad.sum(0))\n",
    "\n",
    "        return dl_dx\n",
    "        \n",
    "    def params(self):\n",
    "        return [(self.w, self.b), (self.dl_dw, self.dl_db)]\n",
    "    \n",
    "    def update_params(self, eta):\n",
    "        self.w = self.w - eta * self.dl_dw\n",
    "        self.b = self.b - eta * self.dl_db\n",
    "        \n",
    "    def reset_gradient(self):\n",
    "        self.dl_dw.zero_()\n",
    "        self.dl_db.zero_()\n",
    "\n",
    "    def reset_params(self):\n",
    "        # Initialize weights\n",
    "        xavier_normal_(self.w)\n",
    "        self.b.normal_(0, self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "latest-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "recovered-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_errors = 0\n",
    "acc_loss = 0\n",
    "x3 = train_input[1]\n",
    "n=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "genetic-combining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "confused-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(100):\n",
    "    pred = x3.max(0)[1].item()\n",
    "    target = 0\n",
    "    if train_target[n].item() == 0 : target = 1\n",
    "    if target != pred : nb_train_errors = nb_train_errors + 1\n",
    "    acc_loss = acc_loss + loss(x3, train_target[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "legitimate-jones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(71.1145)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "immune-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *modules):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.module_lst = []\n",
    "        for module in modules:\n",
    "            self.module_lst.append(module)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for module in self.module_lst:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        for module in reversed(self.module_lst):\n",
    "            grad = module.backward(grad)\n",
    "        return grad\n",
    "    \n",
    "    def update_params(self, eta):\n",
    "        for module in self.module_lst:\n",
    "            module.update_params(eta)\n",
    "            \n",
    "    def params(self):\n",
    "        lst = []\n",
    "        for module in self.module_lst:\n",
    "            lst.append(module.params())\n",
    "        return lst\n",
    "    \n",
    "    def reset_gradient(self):\n",
    "        for module in self.module_lst:\n",
    "            module.reset_gradient()\n",
    "        return\n",
    "    \n",
    "    def reset_params(self):\n",
    "        for module in self.module_lst:\n",
    "            module.reset_params()\n",
    "        return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "seventh-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x.clamp(min=0)\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        ds_dx = (torch.sign(self.x) + 1) / 2\n",
    "        dl_dx = ds_dx * grad\n",
    "        return dl_dx\n",
    "    \n",
    "    def update_params(self, eta):\n",
    "        return\n",
    "    \n",
    "    def reset_gradient(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wireless-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, v, t):\n",
    "        return (v - t).pow(2).sum()\n",
    "    \n",
    "    def backward(self, v, t):\n",
    "        return 2 * (v - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x.tanh()\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        ds_dx = 4 * (self.x.exp() + self.x.mul(-1).exp()).pow(-2)\n",
    "        dl_dx = ds_dx*grad\n",
    "        return dl_dx\n",
    "        \n",
    "    def params(self):\n",
    "        return []\n",
    "    \n",
    "    def update_params(self, eta):\n",
    "        return\n",
    "    \n",
    "    def reset_gradient(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-wilson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-section",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-software",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dutch-medicaid",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hispanic-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disk_set(N=1000):\n",
    "    \n",
    "    # Generate train sets of 2 uniform distributions on [0,1]x[0,1]\n",
    "    input = torch.empty(N, 2).uniform_(0, 1)\n",
    "    \n",
    "    recenter = torch.tensor([0.5, 0.5]) # to act as if the train data was centered around 0, to ease the following computation\n",
    "    \n",
    "    # Generate the target tensors filled with 1 if datapoint is inside of specific circle\n",
    "    target = (-(input - recenter).pow(2).sum(1).sqrt().sub(1 / math.sqrt(2 * math.pi))).sign().add(1).div(2).long()\n",
    "    \n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wired-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "train_input, train_target = generate_disk_set(N)\n",
    "test_input, test_target = generate_disk_set(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-calgary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-indiana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-catalyst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-adaptation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-blackjack",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-jordan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-pizza",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-setup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-apollo",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
