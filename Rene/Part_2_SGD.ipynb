{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quality-breeding",
   "metadata": {},
   "source": [
    "# Mini deep-learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "convertible-bullet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x208f6106e88>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import empty\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-possibility",
   "metadata": {},
   "source": [
    "### Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "precious-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    '''\n",
    "    Compute forward pass from an input tensor and return a tensor\n",
    "    or a tuple of tensors as output\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        '''\n",
    "        should get as input a tensor or a tuple of tensors containing the \n",
    "        gradient of the loss with respect to the module’s output, accumulate \n",
    "        the gradient wrt the parameters, and return a tensor or a tuple of\n",
    "        tensors containing the gradient of the loss wrt the module’s input.\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def params(self):\n",
    "        '''\n",
    "        param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor\n",
    "        of same size. This list should be empty for parameterless modules (e.g. ReLU).\n",
    "\n",
    "        '''\n",
    "        return []\n",
    "        \n",
    "    def reset_params(self):\n",
    "        return\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "from torch.nn.init import xavier_normal_, xavier_normal\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.epsilon = 1e-3\n",
    "        self.x = 0\n",
    "\n",
    "        # Initialize weights\n",
    "        self.w = xavier_normal_(torch.empty(self.dim_out, self.dim_in))\n",
    "        self.b = torch.empty(self.dim_out).normal_(0, self.epsilon)\n",
    "\n",
    "        # Initialize gradient\n",
    "        self.dl_dw = torch.empty(self.w.size())\n",
    "        self.dl_db = torch.empty(self.b.size())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.x.mm(self.w.t()) + self.b\n",
    "\n",
    "\n",
    "    def backward(self, grad):\n",
    "        ds_dx = self.w.t()\n",
    "\n",
    "        # do the same for every batch (batch dim becomes 1)\n",
    "        dl_dx = ds_dx @ grad.t()\n",
    "\n",
    "        # put batch dim back to 0\n",
    "        dl_dx = dl_dx.t()\n",
    "\n",
    "        # sum over all the outer product between (grad_1 * x_1^T) (_1 denotes not using mini-batches)\n",
    "        self.dl_dw.add_(grad.t() @ self.x)\n",
    "\n",
    "        # sum over the batch\n",
    "        self.dl_db.add_(grad.sum(0))\n",
    "\n",
    "        return dl_dx\n",
    "        \n",
    "    def params(self):\n",
    "        return [(self.w, self.b), (self.dl_dw, self.dl_db)]\n",
    "    \n",
    "    def update_params(self, eta):\n",
    "        self.w = self.w - eta * self.dl_dw\n",
    "        self.b = self.b - eta * self.dl_db\n",
    "        \n",
    "    def reset_gradient(self):\n",
    "        self.dl_dw.zero_()\n",
    "        self.dl_db.zero_()\n",
    "\n",
    "    def reset_params(self):\n",
    "        # Initialize weights\n",
    "        xavier_normal_(self.w)\n",
    "        self.b.normal_(0, self.epsilon)\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *modules):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.module_lst = []\n",
    "        for module in modules:\n",
    "            self.module_lst.append(module)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for module in self.module_lst:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        for module in reversed(self.module_lst):\n",
    "            grad = module.backward(grad)\n",
    "        return grad\n",
    "    \n",
    "    def update_params(self, eta):\n",
    "        for module in self.module_lst:\n",
    "            module.update_params(eta)\n",
    "            \n",
    "    def params(self):\n",
    "        lst = []\n",
    "        for module in self.module_lst:\n",
    "            lst.append(module.params())\n",
    "        return lst\n",
    "    \n",
    "    def reset_gradient(self):\n",
    "        for module in self.module_lst:\n",
    "            module.reset_gradient()\n",
    "        return\n",
    "    \n",
    "    def reset_params(self):\n",
    "        for module in self.module_lst:\n",
    "            module.reset_params()\n",
    "            \n",
    "###########################################################################################################\n",
    "\n",
    "def dReLU(x):\n",
    "    s = x.clone()\n",
    "    s[x>0] = 1\n",
    "    s[x<=0] = 0\n",
    "    return s\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x.clamp(min=0)\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        #ds_dx = dReLU(self.x)\n",
    "        ds_dx = (torch.sign(self.x) + 1)/2\n",
    "        dl_dx = ds_dx*grad\n",
    "        return dl_dx\n",
    "    \n",
    "    def update_params(self, eta):\n",
    "        return\n",
    "    \n",
    "    def reset_gradient(self):\n",
    "        return\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x.tanh()\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        ds_dx = 4 * (self.x.exp() + self.x.mul(-1).exp()).pow(-2)\n",
    "        dl_dx = ds_dx*grad\n",
    "        return dl_dx\n",
    "        \n",
    "    def params(self):\n",
    "        return []\n",
    "    \n",
    "    def update_params(self, eta):\n",
    "        return\n",
    "    \n",
    "    def reset_gradient(self):\n",
    "        return\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, v, t):\n",
    "        return (v - t).pow(2).sum()\n",
    "    \n",
    "    def backward(self, v, t):\n",
    "        return 2 * (v - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "selected-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe no need to implement ourselves (no written in the project pdf at least, but reward for 'originality')\n",
    "# From https://towardsdatascience.com/on-implementing-deep-learning-library-from-scratch-in-python-c93c942710a8\n",
    "\n",
    "class Optimizer(object):\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "    \n",
    "    def step(self): \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zeroGrad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = 0.\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, parameters, lr = .001, weight_decay = 0.0, momentum = .9):\n",
    "        super().__init__(parameters)\n",
    "        self.lr           = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum     = momentum\n",
    "        self.velocity     = []\n",
    "        for p in parameters:\n",
    "            self.velocity.append(np.zeros_like(p.grad))\n",
    "\n",
    "    def step(self):\n",
    "        for p,v in zip(self.parameters, self.velocity):\n",
    "            v = self.momentum * v + p.grad + self.weight_decay * p.data\n",
    "            p.data = p.data-self.lr * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pediatric-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, input, target, mini_batch_size):\n",
    "    nb_error = 0\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output = model.forward(input.narrow(0, b, mini_batch_size))\n",
    "        pred = output.max(1)[1]\n",
    "        batch_error = (pred - target.narrow(0, b, mini_batch_size)).abs().sum()\n",
    "        nb_error += batch_error\n",
    "    return 100 * (1 - nb_error.item() / len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tribal-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(target):\n",
    "    one_hot_target = torch.zeros(target.shape[0], 2)\n",
    "    for i in range(one_hot_target.shape[0]):\n",
    "        if target[i] == 0:\n",
    "            one_hot_target[i, 0] = 1\n",
    "            one_hot_target[i, 1] = -1\n",
    "        else:\n",
    "            one_hot_target[i, 1] = 1\n",
    "            one_hot_target[i, 0] = -1\n",
    "    return one_hot_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-drama",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scenic-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disk_set(N=1000):\n",
    "    \n",
    "    # Generate train sets of 2 uniform distributions on [0,1]x[0,1]\n",
    "    train_input = torch.empty(N, 2).uniform_(0, 1)\n",
    "    test_input = torch.empty(N, 2).uniform_(0, 1)\n",
    "    \n",
    "    recenter = torch.tensor([0.5, 0.5]) # to act as if the train data was centered around 0, to ease the following computation\n",
    "    \n",
    "    # Generate the target tensors filled with 1 if datapoint is inside of specific circle\n",
    "    train_target = (-(train_input - recenter).pow(2).sum(1).sqrt().sub(1 / math.sqrt(2 * math.pi))).sign().add(1).div(2).int()\n",
    "    test_target = (-(train_input - recenter).pow(2).sum(1).sqrt().sub(1 / math.sqrt(2 * math.pi))).sign().add(1).div(2).int()\n",
    "    \n",
    "    return train_input, test_input, train_target, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "center-stanley",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9462, 0.9302],\n",
       "         [0.0047, 0.8419],\n",
       "         [0.5731, 0.0127],\n",
       "         ...,\n",
       "         [0.4621, 0.7786],\n",
       "         [0.4808, 0.2043],\n",
       "         [0.2455, 0.9814]]),\n",
       " tensor([[0.1802, 0.3494],\n",
       "         [0.5447, 0.4390],\n",
       "         [0.1462, 0.5370],\n",
       "         ...,\n",
       "         [0.8781, 0.3767],\n",
       "         [0.4319, 0.5806],\n",
       "         [0.9200, 0.2099]]),\n",
       " tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "         0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "         1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "         1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "         1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "         0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "         0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "         0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "         0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "         0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "         1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "         1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "         1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "         0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "         0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "         0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "         0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0], dtype=torch.int32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000\n",
    "train_input, test_input, train_target, test_target = generate_disk_set(N)\n",
    "train_input, test_input, train_target, test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-contractor",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "automated-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of constants\n",
    "input_units = 2\n",
    "output_units = 1\n",
    "hidden_units = 25\n",
    "nb_epochs = 100\n",
    "test_size = 1000\n",
    "train_size = 1000\n",
    "\n",
    "model_1 = Sequential(\n",
    "            Linear(input_units, hidden_units),\n",
    "            ReLU(),\n",
    "            Linear(hidden_units, hidden_units),\n",
    "            Tanh(),\n",
    "            Linear(hidden_units, output_units),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-malta",
   "metadata": {},
   "source": [
    "### Fonction train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "valuable-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, nb_epochs, mini_batch_size, criterion=MSELoss(), eta=1e-6):\n",
    "    model.reset_params()\n",
    "    for e in range(nb_epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            \n",
    "            # forward pass\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            model.reset_gradient()\n",
    "            model.backward(criterion.backward(output, train_target.narrow(0, b, mini_batch_size)))\n",
    "            model.update_params(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "sudden-planning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.narrow(0, 1, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "structured-engagement",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4106e-01],\n",
       "        [-1.7337e-04],\n",
       "        [ 7.2474e-02],\n",
       "        [ 1.4133e-01],\n",
       "        [ 1.2738e-01],\n",
       "        [ 4.9601e-02],\n",
       "        [ 1.1673e-01],\n",
       "        [ 1.7925e-01],\n",
       "        [ 2.0086e-01],\n",
       "        [ 1.2208e-01],\n",
       "        [ 1.9252e-01],\n",
       "        [ 8.3029e-03],\n",
       "        [ 6.4162e-02],\n",
       "        [ 1.0196e-01],\n",
       "        [ 1.1079e-01],\n",
       "        [ 5.6222e-02],\n",
       "        [ 1.2798e-01],\n",
       "        [ 1.5943e-01],\n",
       "        [ 4.3864e-02],\n",
       "        [ 1.0029e-01],\n",
       "        [ 1.8674e-02],\n",
       "        [ 2.7410e-02],\n",
       "        [ 3.7596e-03],\n",
       "        [ 6.0798e-02],\n",
       "        [ 1.2142e-01],\n",
       "        [ 1.0152e-02],\n",
       "        [ 8.6384e-02],\n",
       "        [ 5.4576e-02],\n",
       "        [ 9.5266e-02],\n",
       "        [ 1.6922e-01],\n",
       "        [ 1.2340e-01],\n",
       "        [ 9.7611e-02],\n",
       "        [ 3.8578e-02],\n",
       "        [ 7.2916e-02],\n",
       "        [ 1.4092e-01],\n",
       "        [ 1.8350e-01],\n",
       "        [ 1.2244e-01],\n",
       "        [ 7.6669e-02],\n",
       "        [ 2.5605e-02],\n",
       "        [ 8.9151e-02],\n",
       "        [ 1.6848e-01],\n",
       "        [ 1.4364e-01],\n",
       "        [ 6.3846e-02],\n",
       "        [ 7.4315e-02],\n",
       "        [ 1.7899e-01],\n",
       "        [ 1.1994e-01],\n",
       "        [ 8.6503e-02],\n",
       "        [ 1.6384e-01],\n",
       "        [ 8.4405e-02],\n",
       "        [ 1.2380e-01],\n",
       "        [ 1.8431e-01],\n",
       "        [ 1.6771e-01],\n",
       "        [ 5.5834e-02],\n",
       "        [ 5.1323e-03],\n",
       "        [ 3.8943e-02],\n",
       "        [ 1.6356e-01],\n",
       "        [ 1.3580e-01],\n",
       "        [ 4.3454e-02],\n",
       "        [ 1.2918e-01],\n",
       "        [ 1.2791e-01],\n",
       "        [ 1.1142e-01],\n",
       "        [ 1.6858e-01],\n",
       "        [ 1.2860e-01],\n",
       "        [ 1.5590e-01],\n",
       "        [ 6.4094e-02],\n",
       "        [ 1.1375e-01],\n",
       "        [ 9.5225e-02],\n",
       "        [ 2.0235e-01],\n",
       "        [ 2.6001e-02],\n",
       "        [ 4.8836e-02],\n",
       "        [ 3.8247e-02],\n",
       "        [ 1.5391e-01],\n",
       "        [ 1.4381e-01],\n",
       "        [ 9.9149e-02],\n",
       "        [ 3.7548e-02],\n",
       "        [ 6.9844e-02],\n",
       "        [ 1.6209e-01],\n",
       "        [ 1.8729e-01],\n",
       "        [ 9.9573e-03],\n",
       "        [ 1.0118e-01],\n",
       "        [ 1.7306e-01],\n",
       "        [ 9.5783e-02],\n",
       "        [ 7.7562e-02],\n",
       "        [ 3.1143e-02],\n",
       "        [ 1.5833e-01],\n",
       "        [ 7.5617e-02],\n",
       "        [ 1.5086e-01],\n",
       "        [ 1.7522e-01],\n",
       "        [ 1.0166e-01],\n",
       "        [ 1.1342e-01],\n",
       "        [ 4.2221e-02],\n",
       "        [ 1.0015e-01],\n",
       "        [ 1.7730e-01],\n",
       "        [ 1.0088e-01],\n",
       "        [ 3.7522e-02],\n",
       "        [ 1.2005e-01],\n",
       "        [ 8.5353e-03],\n",
       "        [ 8.0041e-02],\n",
       "        [ 1.2614e-01],\n",
       "        [ 4.8900e-02]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.forward(train_input.narrow(0, 1, mini_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "sapphire-heating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-1.3174e-01,  1.3679e-01,  1.0866e-01, -6.8916e-02,  1.6267e-02,\n",
       "         1.0598e-01,  2.8423e-02, -1.2706e-01,  5.5364e-03,  3.0729e-02,\n",
       "         1.2771e-02,  1.3280e-01,  2.0914e-01,  1.8903e-01,  1.0594e-01,\n",
       "         2.0135e-01,  1.5535e-01,  1.2017e-01,  1.4459e-01,  9.1102e-02,\n",
       "         2.4246e-03,  6.6946e-02,  1.4694e-01, -4.2813e-02,  1.1405e-01,\n",
       "         1.2858e-01,  1.5123e-01, -1.8841e-02, -8.8439e-02, -1.1445e-01,\n",
       "         1.2750e-01,  9.7018e-02,  1.7609e-01,  7.4630e-02, -6.5555e-02,\n",
       "         1.1453e-01, -1.0267e-01,  1.9152e-01,  3.0462e-02,  2.1661e-01,\n",
       "        -1.3048e-01,  1.2642e-01,  1.8533e-01,  3.9303e-02,  1.6528e-02,\n",
       "         3.6383e-02,  2.6786e-02,  7.4169e-02,  1.7874e-01, -1.2727e-02,\n",
       "        -4.8406e-02,  1.3278e-02,  3.3684e-02,  2.1524e-01,  1.6768e-01,\n",
       "        -7.1598e-02,  1.3777e-01,  5.4457e-02,  1.4134e-02, -1.0886e-01,\n",
       "         1.2966e-01,  2.9005e-05,  1.1616e-01, -7.1770e-02,  1.3753e-01,\n",
       "         1.0086e-01, -4.5839e-02,  1.2525e-01,  1.2561e-01,  7.1874e-02,\n",
       "         5.4436e-02, -8.7871e-02,  1.1118e-01,  1.8772e-01,  1.2955e-01,\n",
       "         2.0384e-01,  8.0894e-02,  1.4545e-01,  3.5991e-02,  1.1327e-01,\n",
       "         1.1257e-01,  9.9885e-02, -6.1945e-02,  1.2728e-01, -3.9130e-02,\n",
       "         9.7286e-03,  1.5517e-01,  1.1748e-01, -2.9698e-02,  4.6681e-02,\n",
       "         4.9822e-02,  1.6071e-01,  5.3621e-02,  1.6863e-01,  2.2744e-01,\n",
       "         5.1929e-02,  1.7915e-01,  4.8017e-02,  9.6235e-02,  1.4269e-01]),\n",
       "indices=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model_1.forward(train_input.narrow(0, 1, mini_batch_size))\n",
    "pred = output.max(1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "monthly-softball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [0.0875],\n",
      "        [0.2360],\n",
      "        [0.1794],\n",
      "        [0.0571],\n",
      "        [0.1661],\n",
      "        [0.3314],\n",
      "        [0.2842],\n",
      "        [0.1741]]) ok tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "model = model_1\n",
    "model.reset_params()\n",
    "output = model.forward(train_input.narrow(0, 1, mini_batch_size))\n",
    "print(output[1:10], 'ok', train_target.narrow(0, 1, mini_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ready-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n",
      "Train accuracy:  51.3\n",
      "Test accuracy: 51.3\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 100\n",
    "train_target_one_hot = one_hot(train_target)\n",
    "\n",
    "for i in range(100):\n",
    "    train_model(model_1, train_input, train_target_one_hot, nb_epochs, mini_batch_size)\n",
    "    # issue is that I put the model_1 output_hidden as 1 instead of 2.. \n",
    "    # train_model(model_1, train_input, train_target.reshape(train_target.size(0),1), nb_epochs, mini_batch_size)\n",
    "    print(\"Train accuracy: \", round(compute_accuracy(model_1, train_input, train_target, mini_batch_size), 2))\n",
    "    print(\"Test accuracy:\", round(compute_accuracy(model_1, test_input, test_target, mini_batch_size), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "macro-hollywood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.narrow(0, 1, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "speaking-reaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2*(model.forward(train_input.narrow(0, 1, mini_batch_size)) - train_target.narrow(0, 1, mini_batch_size).reshape(100,1))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "expanded-malta",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0427],\n",
       "        [0.0049],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0128],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0395],\n",
       "        [0.0420],\n",
       "        [0.0249],\n",
       "        [0.0000],\n",
       "        [0.0429],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0274],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0081],\n",
       "        [0.0454],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0377],\n",
       "        [0.0164],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0419],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0312],\n",
       "        [0.0000],\n",
       "        [0.0354],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0335],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0244],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0695],\n",
       "        [0.0385],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0179],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0293],\n",
       "        [0.0014],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0249],\n",
       "        [0.0250],\n",
       "        [0.0377],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0046],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0272],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0133],\n",
       "        [0.0000],\n",
       "        [0.0165],\n",
       "        [0.0612],\n",
       "        [0.0000],\n",
       "        [0.0557],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0247]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_input.narrow(0, 1, mini_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "exotic-standard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.narrow(0, 1, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "guilty-corner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "        [ 0.0854,  0.0854, -1.9146,  ..., -1.9146, -1.9146, -1.9146],\n",
       "        [ 0.0097,  0.0097, -1.9903,  ..., -1.9903, -1.9903, -1.9903],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "        [ 0.0000,  0.0000, -2.0000,  ..., -2.0000, -2.0000, -2.0000],\n",
       "        [ 0.0493,  0.0493, -1.9507,  ..., -1.9507, -1.9507, -1.9507]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*(model.forward(train_input.narrow(0, 1, mini_batch_size)) - train_target.narrow(0, 1, mini_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-things",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-venice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-language",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-sauce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-smoke",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-answer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-penalty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-willow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
